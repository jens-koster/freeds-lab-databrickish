{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Wikipedia pageviews Bronze\n",
    "Executes by run_date as string formatted 'yyyy-mm-dd\n",
    "Assumes s3 folder structure: \"s3a://data/wikipedia_pageviews/YYYY/YYYY-MM/DD/pageviews-YYYYMMDD-hhmmss.gz\"\n",
    "Performs: \n",
    "* ingestion\n",
    "* column naming\n",
    "* column casting\n",
    "* get the date from the filename\n",
    "* partitioning\n",
    "* storing as delta table\n",
    "\n",
    "Note: there will be a bunch of warnings and some exceptions from the Hive catalog, that's because it doesn't support schema updates. Everything works just fine anyway.\n",
    "In databricks the Unity Catalog is used, but it's proproetary so we'll have to make do with Hive Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "execution_date = \"2025-05-15T01:00:00+00:00\"\n",
    "execution_id = f\"wikipedia_pageviews_bronze-{dt.datetime.now():%Y%m%d-%H0000}\"\n",
    "full_refresh = False\n",
    "bronze_db = \"bronze\"\n",
    "root_prefix = \"wikipedia_pageviews\"\n",
    "data_bucket = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Bronze pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from freeds.s3 import as_urls, list_files, list_files_for_dates\n",
    "from freeds.spark import get_spark_session, show_spark_info\n",
    "from freeds.utils import date_range\n",
    "from pyspark.sql.functions import col, input_file_name, substring, to_date\n",
    "from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n",
    "\n",
    "# s3_path = \"s3a://data/wikipedia_pageviews/2025/2025-03/*/*.gz\"\n",
    "# s3_path = \"s3a://data/wikipedia_pageviews/2025/2025-03/11/pageviews-20250311-220000.gz\"\n",
    "# s3_path = \"s3a://data/test.csv\"\n",
    "\n",
    "\n",
    "if not os.environ.get(\"SPARK_CONF_DIR\"):\n",
    "    os.environ[\"SPARK_CONF_DIR\"] = \"/opt/freeds/spark/conf\"\n",
    "\n",
    "print(\"Getting a spark session\")\n",
    "spark = get_spark_session(execution_id)\n",
    "show_spark_info(spark)\n",
    "\n",
    "print(f\"Creating the {bronze_db} database\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {bronze_db}\")\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(name=\"domain_code\", dataType=StringType(), nullable=True),\n",
    "        StructField(\"page_title\", StringType(), True),\n",
    "        StructField(\"count_views\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "if full_refresh:\n",
    "    file_spec = list_files(prefix=root_prefix, bucket_name=data_bucket)\n",
    "else:\n",
    "    run_dates = date_range(execution_date=execution_date, length=2)\n",
    "    file_spec = list_files_for_dates(dates=run_dates, root_prefix=root_prefix, bucket_name=data_bucket)\n",
    "\n",
    "file_spec = as_urls(file_spec, bucket_name=data_bucket)\n",
    "\n",
    "print(f\"Loading data, found {len(file_spec)} files for {'full refresh' if full_refresh else run_dates}.\")\n",
    "\n",
    "# this only helps if part of the files are missing\n",
    "spark.conf.set(\"spark.sql.files.ignoreMissingFiles\", \"true\")\n",
    "# in our case all files were missing, wrap in try/except\n",
    "reader = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"delimiter\", \" \")\n",
    "    .option(\"header\", \"false\")\n",
    "    .option(\"inferSchema\", \"false\")\n",
    "    .schema(schema)\n",
    ")\n",
    "\n",
    "data = reader.load(file_spec)\n",
    "print(f\"Files to load in spark:{len(data.inputFiles())}, listing first 10:\")\n",
    "for f in data.inputFiles()[:10]:\n",
    "    print(f)\n",
    "\n",
    "data_enriched = (\n",
    "    data.na.drop(subset=[\"domain_code\"])\n",
    "    .filter(~col(\"page_title\").contains(\":\"))\n",
    "    .filter(~col(\"page_title\").isin(\"-\", \"Main_Page\", \"Forside\", \"Hauptseite\", \"wiki.phtml\"))\n",
    "    .withColumn(\"country_code\", substring(col(\"domain_code\"), 1, 2))\n",
    "    .filter(col(\"domain_code\").isin(\"sv\", \"dk\", \"no\", \"de\", \"en\"))\n",
    "    .withColumn(\"count_views\", col(\"count_views\").cast(IntegerType()))\n",
    "    .withColumn(\"file_name\", input_file_name())\n",
    "    .withColumn(\"date\", to_date(substring(col(\"file_name\"), -18, 8), \"yyyyMMdd\"))\n",
    ")\n",
    "\n",
    "table_name = f\"{bronze_db}.wikipedia_page_reads\"\n",
    "# silence a few bulky hive catalog warnings\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "if not full_refresh:\n",
    "    spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "(\n",
    "    data_enriched.write.mode(\"overwrite\")  # Options: 'overwrite', 'append', 'ignore', 'error' (default)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .format(\"delta\")  # Options: 'parquet', 'csv', 'json', 'orc', etc.\n",
    "    .partitionBy(\"date\")\n",
    "    .saveAsTable(table_name)\n",
    ")\n",
    "print(f\"Updated delta table: {table_name}\")\n",
    "spark.stop()\n",
    "print(f\"All done: {execution_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
