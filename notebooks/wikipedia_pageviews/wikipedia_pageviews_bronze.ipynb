{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia pageviews Bronze\n",
    "Executes by run_date as string formatted 'yyyy-mm-dd\n",
    "Assumes s3 folder structure: \"s3a://data/wikipedia_pageviews/YYYY/YYYY-MM/DD/pageviews-YYYYMMDD-hhmmss.gz\"\n",
    "Performs: \n",
    "* ingestion\n",
    "* column naming\n",
    "* column casting\n",
    "* get the date from the filename\n",
    "* partitioning\n",
    "* storing as delta table\n",
    "\n",
    "Note: there will be a bunch of warnings and some exceptions from the Hive catalog, that's because it doesn't support schema updates. Everything works just fine anyway.\n",
    "In databricks the Unity Catalog is used, but it's proproetary so we'll have to make do with Hive Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "run_date = \"2025-03-12\"\n",
    "spark_name_base = \"wikipedia_pageviews_bronze\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard stuff that should be a package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "import pyspark\n",
    "import requests  # type: ignore\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n",
    "\n",
    "os.environ[\"SPARK_CONF_DIR\"] = \"/opt/tfds/spark/conf\"\n",
    "\n",
    "\n",
    "def get_config(config_name):\n",
    "    \"\"\"Get config from tfds-config server.\"\"\"\n",
    "    config_server_url = os.environ.get(\"TFDS_CONFIG_URL\")\n",
    "    if config_server_url is None:\n",
    "        config_server_url = \"http://tfds-config:8005/api/configs\"\n",
    "\n",
    "    config_url = config_server_url + \"/\" + config_name\n",
    "\n",
    "    print(f\"retrieving {config_name} config from {config_url}\")\n",
    "    response = requests.get(config_url)\n",
    "    response.raise_for_status()\n",
    "    if response.json() is None:\n",
    "        raise ValueError(f\"Config '{config_name}' not found. config server response: {response.text}\")\n",
    "    cfg = response.json().get(\"config\")\n",
    "    if cfg is None:\n",
    "        raise ValueError(\n",
    "            f\"Config '{config_name}' does not have a 'config' key. Config server response: {response.text}\"\n",
    "        )\n",
    "\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def get_spark_session(app_name_base: str) -> SparkSession:\n",
    "    \"\"\"Get spark client for s3.\"\"\"\n",
    "    s3_cfg = get_config(\"s3\")\n",
    "\n",
    "    app_name = f\"{app_name_base}-{datetime.datetime.now():%Y%m%d-%H%M%S}\"\n",
    "\n",
    "    conf = (\n",
    "        pyspark.conf.SparkConf()\n",
    "        .setAppName(app_name)\n",
    "        # s3 secrets\n",
    "        .set(\"spark.hadoop.fs.s3a.access.key\", s3_cfg[\"access_key\"])\n",
    "        .set(\"spark.hadoop.fs.s3a.secret.key\", s3_cfg[\"secret_key\"])\n",
    "        .set(\"spark.task.maxFailures\", \"1\")\n",
    "        # .setMaster(\"local[*]\")\n",
    "    )\n",
    "    builder = pyspark.sql.SparkSession.builder.config(conf=conf)\n",
    "    spark_session = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "    return spark_session\n",
    "\n",
    "\n",
    "def show_cfg(spark_session: SparkSession):\n",
    "    \"\"\"Print out the spark config.\"\"\"\n",
    "    cfg = spark_session.sparkContext.getConf().getAll()\n",
    "    for key, value in cfg:\n",
    "        if key in (\n",
    "            \"spark.submit.pyFiles\",\n",
    "            \"spark.driver.extraJavaOptions\",\n",
    "            \"park.app.initial.jar.urls\",\n",
    "            \"spark.files\",\n",
    "            \"spark.repl.local.jars\",\n",
    "            \"spark.app.initial.file.urls\" \"spark.executor.extraJavaOption\",\n",
    "            \"spark.app.initial.jar.urls\" \"spark.app.initial.file.urls\",\n",
    "        ):\n",
    "            print(key)\n",
    "            for csv in value.split(\",\"):\n",
    "                print(\"    \" + str(csv))\n",
    "        else:\n",
    "            print(f\"{key} = {value}\")\n",
    "\n",
    "\n",
    "def print_spark_info(sc: SparkSession):\n",
    "    \"\"\"Print some spark info.\"\"\"\n",
    "    cfg: pyspark.SparkConf = sc.sparkContext.getConf()\n",
    "    print(f'==== spark app: {cfg.get(\"spark.app.name\")} ====')\n",
    "    print(f'Spark master: {cfg.get(\"spark.master\")}')\n",
    "    print(f'Delta lake location: {cfg.get(\"spark.sql.warehouse.dir\")}')\n",
    "    print(f'S3 endpoint: {cfg.get(\"spark.hadoop.fs.s3a.endpoint\")}')\n",
    "\n",
    "    dbs = sc.catalog.listDatabases()\n",
    "    print(\"Databases:\")\n",
    "    for db in dbs:\n",
    "        print(db.name)\n",
    "        tables = sc.catalog.listTables(db.name)\n",
    "        for tbl in tables:\n",
    "            print(f\"    {tbl.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bronze pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, input_file_name, substring, to_date\n",
    "\n",
    "# s3_path = \"s3a://data/wikipedia_pageviews/2025/2025-03/*/*.gz\"\n",
    "# s3_path = \"s3a://data/wikipedia_pageviews/2025/2025-03/11/pageviews-20250311-220000.gz\"\n",
    "# s3_path = \"s3a://data/test.csv\"\n",
    "s3_path = f\"s3a://data/wikipedia_pageviews/{run_date[:4]}/{run_date[:7]}/{run_date[-2:]}/*.gz\"\n",
    "\n",
    "spark = get_spark_session(spark_name_base)\n",
    "print_spark_info(spark)\n",
    "\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS bronze\")\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(name=\"domain_code\", dataType=StringType(), nullable=True),\n",
    "        StructField(\"page_title\", StringType(), True),\n",
    "        StructField(\"count_views\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Loading data:{s3_path}\")\n",
    "data = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"delimiter\", \" \")\n",
    "    .option(\"header\", \"false\")\n",
    "    .option(\"inferSchema\", \"false\")\n",
    "    .schema(schema)\n",
    "    .load(s3_path)\n",
    ")\n",
    "print(\"Files loaded:\")\n",
    "for f in data.inputFiles():\n",
    "    print(f)\n",
    "\n",
    "\n",
    "data_enriched = (\n",
    "    data.na.drop(subset=[\"domain_code\"])\n",
    "    .filter(~col(\"page_title\").contains(\":\"))\n",
    "    .filter(~col(\"page_title\").isin(\"-\", \"Main_Page\", \"Forside\", \"Hauptseite\", \"wiki.phtml\"))\n",
    "    .withColumn(\"country_code\", substring(col(\"domain_code\"), 1, 2))\n",
    "    .filter(col(\"domain_code\").isin(\"sv\", \"dk\", \"no\", \"de\", \"en\"))\n",
    "    .withColumn(\"count_views\", col(\"count_views\").cast(IntegerType()))\n",
    "    .withColumn(\"file_name\", input_file_name())\n",
    "    .withColumn(\"date\", to_date(substring(col(\"file_name\"), -18, 8), \"yyyyMMdd\"))\n",
    ")\n",
    "\n",
    "table_name = \"bronze.wikipedia_page_reads\"\n",
    "# silence a few bulky hive catalog warnings\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "(\n",
    "    data_enriched.write.mode(\"overwrite\")  # Options: 'overwrite', 'append', 'ignore', 'error' (default)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .format(\"delta\")  # Options: 'parquet', 'csv', 'json', 'orc', etc.\n",
    "    .partitionBy(\"date\")\n",
    "    .saveAsTable(table_name)\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(f\"Updated delta table: {table_name}\")\n",
    "print_spark_info(spark)\n",
    "spark.stop()\n",
    "print(\"All done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFDS 3.8.10",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
