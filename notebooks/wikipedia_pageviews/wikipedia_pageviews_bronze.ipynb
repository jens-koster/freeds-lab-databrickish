{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia pageviews Bronze\n",
    "Executes by run_date as string formatted 'yyyy-mm-dd\n",
    "Assumes s3 folder structure: \"s3a://data/wikipedia_pageviews/YYYY/YYYY-MM/DD/pageviews-YYYYMMDD-hhmmss.gz\"\n",
    "Performs: \n",
    "* ingestion\n",
    "* column naming\n",
    "* column casting\n",
    "* get the date from the filename\n",
    "* partitioning\n",
    "* storing as delta table\n",
    "\n",
    "Note: there will be a bunch of warnings and some exceptions from the Hive catalog, that's because it doesn't support schema updates. Everything works just fine anyway.\n",
    "In databricks the Unity Catalog is used, but it's proproetary so we'll have to make do with Hive Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "execution_date = \"2025-05-15T01:00:00+00:00\"\n",
    "execution_id = f\"wikipedia_pageviews_bronze-{dt.datetime.now():%Y%m%d-%H0000}\"\n",
    "full_refresh = False\n",
    "bronze_db = \"bronze\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard stuff that should be a package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import os\n",
    "\n",
    "import pyspark\n",
    "import requests  # type: ignore\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "print(f\"Starting notebook execution: {execution_id}\")\n",
    "\n",
    "if not os.environ.get(\"SPARK_CONF_DIR\"):\n",
    "    os.environ[\"SPARK_CONF_DIR\"] = \"/opt/tfds/spark/conf\"\n",
    "\n",
    "\n",
    "def get_date():\n",
    "    \"\"\"Parse the exeuction date to a datetime object.\"\"\"\n",
    "    return dt.datetime.fromisoformat(execution_date)\n",
    "\n",
    "\n",
    "def get_config(config_name):\n",
    "    \"\"\"Get config from tfds-config server.\"\"\"\n",
    "    config_server_url = os.environ.get(\"TFDS_CONFIG_URL\")\n",
    "    if config_server_url is None:\n",
    "        config_server_url = \"http://tfds-config:8005/api/configs\"\n",
    "\n",
    "    config_url = config_server_url + \"/\" + config_name\n",
    "\n",
    "    print(f\"retrieving {config_name} config from {config_url}\")\n",
    "    response = requests.get(config_url)\n",
    "    response.raise_for_status()\n",
    "    if response.json() is None:\n",
    "        raise ValueError(f\"Config '{config_name}' not found. config server response: {response.text}\")\n",
    "    cfg = response.json().get(\"config\")\n",
    "    if cfg is None:\n",
    "        raise ValueError(\n",
    "            f\"Config '{config_name}' does not have a 'config' key. Config server response: {response.text}\"\n",
    "        )\n",
    "\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def get_spark_session(app_name: str) -> SparkSession:\n",
    "    \"\"\"Get spark client for s3.\"\"\"\n",
    "    s3_cfg = get_config(\"s3\")\n",
    "\n",
    "    conf = (\n",
    "        pyspark.conf.SparkConf()\n",
    "        .setAppName(app_name)\n",
    "        # s3 secrets\n",
    "        .set(\"spark.hadoop.fs.s3a.access.key\", s3_cfg[\"access_key\"])\n",
    "        .set(\"spark.hadoop.fs.s3a.secret.key\", s3_cfg[\"secret_key\"])\n",
    "        .set(\"spark.task.maxFailures\", \"1\")\n",
    "        # .setMaster(\"local[*]\")\n",
    "    )\n",
    "    builder = pyspark.sql.SparkSession.builder.config(conf=conf)\n",
    "    spark_session = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "    return spark_session\n",
    "\n",
    "\n",
    "def show_cfg(spark_session: SparkSession):\n",
    "    \"\"\"Print out the spark config.\"\"\"\n",
    "    cfg = spark_session.sparkContext.getConf().getAll()\n",
    "    for key, value in cfg:\n",
    "        if key in (\n",
    "            \"spark.submit.pyFiles\",\n",
    "            \"spark.driver.extraJavaOptions\",\n",
    "            \"park.app.initial.jar.urls\",\n",
    "            \"spark.files\",\n",
    "            \"spark.repl.local.jars\",\n",
    "            \"spark.app.initial.file.urls\" \"spark.executor.extraJavaOption\",\n",
    "            \"spark.app.initial.jar.urls\" \"spark.app.initial.file.urls\",\n",
    "        ):\n",
    "            print(key)\n",
    "            for csv in value.split(\",\"):\n",
    "                print(\"    \" + str(csv))\n",
    "        else:\n",
    "            print(f\"{key} = {value}\")\n",
    "\n",
    "\n",
    "def print_spark_info(sc: SparkSession):\n",
    "    \"\"\"Print some spark info.\"\"\"\n",
    "    cfg: pyspark.SparkConf = sc.sparkContext.getConf()\n",
    "    print(f'==== spark app: {cfg.get(\"spark.app.name\")} ====')\n",
    "    print(f'Spark master: {cfg.get(\"spark.master\")}')\n",
    "    print(f'Delta lake location: {cfg.get(\"spark.sql.warehouse.dir\")}')\n",
    "    print(f'S3 endpoint: {cfg.get(\"spark.hadoop.fs.s3a.endpoint\")}')\n",
    "    print(f'Custom config file status: {cfg.get(\"spark.signal.config.value\")}')\n",
    "\n",
    "    dbs = sc.catalog.listDatabases()\n",
    "    print(\"Databases:\")\n",
    "    for db in dbs:\n",
    "        print(db.name)\n",
    "        tables = sc.catalog.listTables(db.name)\n",
    "        for tbl in tables:\n",
    "            print(f\"    {tbl.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bronze pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, input_file_name, substring, to_date\n",
    "\n",
    "\n",
    "def make_s3_path(date):\n",
    "    \"\"\"Make s3 path for the given date.\"\"\"\n",
    "    year = date.strftime(\"%Y\")\n",
    "    month = date.strftime(\"%m\")\n",
    "    day = date.strftime(\"%d\")\n",
    "    return f\"s3a://data/wikipedia_pageviews/{year}/{year}-{month}/{day}/*.gz\"\n",
    "\n",
    "\n",
    "# s3_path = \"s3a://data/wikipedia_pageviews/2025/2025-03/*/*.gz\"\n",
    "# s3_path = \"s3a://data/wikipedia_pageviews/2025/2025-03/11/pageviews-20250311-220000.gz\"\n",
    "# s3_path = \"s3a://data/test.csv\"\n",
    "\n",
    "run_dates = [get_date(), get_date() - dt.timedelta(days=1)]\n",
    "\n",
    "print(\"Getting a spark session\")\n",
    "spark = get_spark_session(execution_id)\n",
    "print_spark_info(spark)\n",
    "\n",
    "print(f\"Creating the {bronze_db} database\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {bronze_db}\")\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(name=\"domain_code\", dataType=StringType(), nullable=True),\n",
    "        StructField(\"page_title\", StringType(), True),\n",
    "        StructField(\"count_views\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "file_spec = [make_s3_path(date) for date in run_dates]\n",
    "print(f\"Loading data: {file_spec}\")\n",
    "\n",
    "# this only helps if part of the files are missing\n",
    "spark.conf.set(\"spark.sql.files.ignoreMissingFiles\", \"true\")\n",
    "# in our case all files were missing, wrap in try/except\n",
    "reader = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"delimiter\", \" \")\n",
    "    .option(\"header\", \"false\")\n",
    "    .option(\"inferSchema\", \"false\")\n",
    "    .schema(schema)\n",
    ")\n",
    "try:\n",
    "    data = reader.load(file_spec)\n",
    "except AnalysisException as e:\n",
    "    if \"Path does not exist\" in str(e):\n",
    "        data = spark.createDataFrame([], schema)\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "if len(data.inputFiles()) == 0:\n",
    "    print(f\"No files found in: {file_spec}\")\n",
    "else:\n",
    "    print(\"Files found:\")\n",
    "    for f in data.inputFiles():\n",
    "        print(f)\n",
    "\n",
    "data_enriched = (\n",
    "    data.na.drop(subset=[\"domain_code\"])\n",
    "    .filter(~col(\"page_title\").contains(\":\"))\n",
    "    .filter(~col(\"page_title\").isin(\"-\", \"Main_Page\", \"Forside\", \"Hauptseite\", \"wiki.phtml\"))\n",
    "    .withColumn(\"country_code\", substring(col(\"domain_code\"), 1, 2))\n",
    "    .filter(col(\"domain_code\").isin(\"sv\", \"dk\", \"no\", \"de\", \"en\"))\n",
    "    .withColumn(\"count_views\", col(\"count_views\").cast(IntegerType()))\n",
    "    .withColumn(\"file_name\", input_file_name())\n",
    "    .withColumn(\"date\", to_date(substring(col(\"file_name\"), -18, 8), \"yyyyMMdd\"))\n",
    ")\n",
    "\n",
    "table_name = f\"{bronze_db}.wikipedia_page_reads\"\n",
    "# silence a few bulky hive catalog warnings\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "if not full_refresh:\n",
    "    spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "(\n",
    "    data_enriched.write.mode(\"overwrite\")  # Options: 'overwrite', 'append', 'ignore', 'error' (default)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .format(\"delta\")  # Options: 'parquet', 'csv', 'json', 'orc', etc.\n",
    "    .partitionBy(\"date\")\n",
    "    .saveAsTable(table_name)\n",
    ")\n",
    "print(f\"Updated delta table: {table_name}\")\n",
    "spark.stop()\n",
    "print(f\"All done: {execution_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFDS 3.8.10",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
