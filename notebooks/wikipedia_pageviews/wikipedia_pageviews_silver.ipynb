{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia pageviews Silver\n",
    "Executes by rundate.\n",
    "Aggregates and ranks pageviews on a daily level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "execution_date = \"2025-05-15T01:00:00+00:00\"\n",
    "execution_id = f\"wikipedia_pageviews_silver-{dt.datetime.now():%Y%m%d-%H0000}\"\n",
    "full_refresh = False\n",
    "silver_db = \"silver\"\n",
    "bronze_db = \"bronze\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard stuff that should be a package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pyspark\n",
    "import requests  # type: ignore\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "print(f\"Starting notebook execution: {execution_id}\")\n",
    "\n",
    "if not os.environ.get(\"SPARK_CONF_DIR\"):\n",
    "    os.environ[\"SPARK_CONF_DIR\"] = \"/opt/tfds/spark/conf\"\n",
    "\n",
    "\n",
    "def get_date():\n",
    "    \"\"\"Parse the execution date to a datetime object.\"\"\"\n",
    "    return dt.datetime.fromisoformat(execution_date)\n",
    "\n",
    "\n",
    "def get_dates(window_size: int = 1):\n",
    "    \"\"\"Return a list of window_size dates back in time from execution_date (inclusive).\"\"\"\n",
    "    start_date = dt.datetime.fromisoformat(execution_date)\n",
    "    return [start_date - dt.timedelta(days=i) for i in reversed(range(window_size))]\n",
    "\n",
    "\n",
    "def get_config(config_name):\n",
    "    \"\"\"Get config from tfds-config server.\"\"\"\n",
    "    config_server_url = os.environ.get(\"TFDS_CONFIG_URL\")\n",
    "    if config_server_url is None:\n",
    "        config_server_url = \"http://tfds-config:8005/api/configs\"\n",
    "\n",
    "    config_url = config_server_url + \"/\" + config_name\n",
    "\n",
    "    print(f\"retrieving {config_name} config from {config_url}\")\n",
    "    response = requests.get(config_url)\n",
    "    response.raise_for_status()\n",
    "    if response.json() is None:\n",
    "        raise ValueError(f\"Config '{config_name}' not found. config server response: {response.text}\")\n",
    "    cfg = response.json().get(\"config\")\n",
    "    if cfg is None:\n",
    "        raise ValueError(\n",
    "            f\"Config '{config_name}' does not have a 'config' key. Config server response: {response.text}\"\n",
    "        )\n",
    "\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def get_spark_session(app_name: str) -> SparkSession:\n",
    "    \"\"\"Get spark client for s3.\"\"\"\n",
    "    s3_cfg = get_config(\"s3\")\n",
    "\n",
    "    conf = (\n",
    "        pyspark.conf.SparkConf()\n",
    "        .setAppName(app_name)\n",
    "        # s3 secrets\n",
    "        .set(\"spark.hadoop.fs.s3a.access.key\", s3_cfg[\"access_key\"])\n",
    "        .set(\"spark.hadoop.fs.s3a.secret.key\", s3_cfg[\"secret_key\"])\n",
    "        .set(\"spark.task.maxFailures\", \"1\")\n",
    "        # .setMaster(\"local[*]\")\n",
    "    )\n",
    "    builder = pyspark.sql.SparkSession.builder.config(conf=conf)\n",
    "    spark_session = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "    return spark_session\n",
    "\n",
    "\n",
    "def show_cfg(spark_session: SparkSession):\n",
    "    \"\"\"Print out the spark config.\"\"\"\n",
    "    cfg = spark_session.sparkContext.getConf().getAll()\n",
    "    for key, value in cfg:\n",
    "        if key in (\n",
    "            \"spark.submit.pyFiles\",\n",
    "            \"spark.driver.extraJavaOptions\",\n",
    "            \"park.app.initial.jar.urls\",\n",
    "            \"spark.files\",\n",
    "            \"spark.repl.local.jars\",\n",
    "            \"spark.app.initial.file.urls\" \"spark.executor.extraJavaOption\",\n",
    "            \"spark.app.initial.jar.urls\" \"spark.app.initial.file.urls\",\n",
    "        ):\n",
    "            print(key)\n",
    "            for csv in value.split(\",\"):\n",
    "                print(\"    \" + str(csv))\n",
    "        else:\n",
    "            print(f\"{key} = {value}\")\n",
    "\n",
    "\n",
    "def print_spark_info(sc: SparkSession):\n",
    "    \"\"\"Print some spark info.\"\"\"\n",
    "    cfg: pyspark.SparkConf = sc.sparkContext.getConf()\n",
    "    print(f'==== spark app: {cfg.get(\"spark.app.name\")} ====')\n",
    "    print(f'Spark master: {cfg.get(\"spark.master\")}')\n",
    "    print(f'Delta lake location: {cfg.get(\"spark.sql.warehouse.dir\")}')\n",
    "    print(f'S3 endpoint: {cfg.get(\"spark.hadoop.fs.s3a.endpoint\")}')\n",
    "    print(f'Custom config file status: {cfg.get(\"spark.signal.config.value\")}')\n",
    "\n",
    "    dbs = sc.catalog.listDatabases()\n",
    "    print(\"Databases:\")\n",
    "    for db in dbs:\n",
    "        print(db.name)\n",
    "        tables = sc.catalog.listTables(db.name)\n",
    "        for tbl in tables:\n",
    "            print(f\"    {tbl.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "source_table_name = f\"{bronze_db}.wikipedia_page_reads\"\n",
    "target_table_name = f\"{silver_db}.wikipedia_page_ranks_100\"\n",
    "\n",
    "spark = get_spark_session(execution_id)\n",
    "\n",
    "partition_list = get_dates(2)\n",
    "\n",
    "filtered_df = spark.table(source_table_name).filter(col(\"date\").isin(partition_list))\n",
    "\n",
    "aggregated_df = filtered_df.groupBy(\"date\", \"page_title\", \"country_code\").agg(\n",
    "    _sum(col(\"count_views\")).alias(\"total_count_views\"),\n",
    ")\n",
    "\n",
    "national_win = Window.partitionBy(\"date\", \"country_code\").orderBy(col(\"total_count_views\").desc())\n",
    "\n",
    "ranked_df = aggregated_df.withColumn(\"national_rank\", row_number().over(national_win))\n",
    "\n",
    "final_df = ranked_df.filter(col(\"national_rank\") <= 100).orderBy(\"date\", \"national_rank\", \"country_code\")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {silver_db}\")\n",
    "if not full_refresh:\n",
    "    spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "(\n",
    "    final_df.write.mode(\"overwrite\")  # Options: 'overwrite', 'append', 'ignore', 'error' (default)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .format(\"delta\")  # Options: 'parquet', 'csv', 'json', 'orc', etc.\n",
    "    .partitionBy(\"date\")\n",
    "    .saveAsTable(target_table_name)\n",
    ")\n",
    "spark.stop()\n",
    "print(f\"All done: {execution_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFDS 3.8.10",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
