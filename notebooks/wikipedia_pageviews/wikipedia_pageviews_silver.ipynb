{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia pageviews Silver\n",
    "Executes by rundate.\n",
    "Aggregates and ranks pageviews on a daily level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "run_date = \"2025-03-11\"\n",
    "spark_name_base = \"wikipedia_pageviews_silver\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard stuff that should be a package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "import pyspark\n",
    "import requests  # type: ignore\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# make sure we read the config and from the same location as a containerized version would.\n",
    "os.environ[\"SPARK_CONF_DIR\"] = \"/opt/tfds/spark/conf\"\n",
    "\n",
    "\n",
    "def get_config(config_name):\n",
    "    \"\"\"Get config from tfds-config server.\"\"\"\n",
    "    config_server_url = os.environ.get(\"TFDS_CONFIG_URL\")\n",
    "    if config_server_url is None:\n",
    "        config_server_url = \"http://tfds-config:8005/api/configs\"\n",
    "\n",
    "    config_url = config_server_url + \"/\" + config_name\n",
    "\n",
    "    print(f\"retrieving {config_name} config from {config_url}\")\n",
    "    response = requests.get(config_url)\n",
    "    response.raise_for_status()\n",
    "    if response.json() is None:\n",
    "        raise ValueError(f\"Config '{config_name}' not found. config server response: {response.text}\")\n",
    "    cfg = response.json().get(\"config\")\n",
    "    if cfg is None:\n",
    "        raise ValueError(\n",
    "            f\"Config '{config_name}' does not have a 'config' key. Config server response: {response.text}\"\n",
    "        )\n",
    "\n",
    "    if config_name == \"s3\" and \"TFDS_S3_URL\" in os.environ.keys():\n",
    "        cfg[\"url\"] = os.environ[\"TFDS_S3_URL\"]\n",
    "    if config_name == \"spark\" and \"TFDS_SPARK_MASTER_URL\" in os.environ.keys():\n",
    "        cfg[\"master_url\"] = os.environ[\"TFDS_SPARK_MASTER_URL\"]\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def get_spark_session(app_name_base: str) -> SparkSession:\n",
    "    \"\"\"Get spark client for s3.\"\"\"\n",
    "    s3_cfg = get_config(\"s3\")\n",
    "\n",
    "    app_name = f\"{app_name_base}-{datetime.datetime.now():%Y%m%d-%H%M%S}\"\n",
    "\n",
    "    conf = (\n",
    "        pyspark.conf.SparkConf()\n",
    "        .setAppName(app_name)\n",
    "        # s3 secrets\n",
    "        .set(\"spark.hadoop.fs.s3a.access.key\", s3_cfg[\"access_key\"])\n",
    "        .set(\"spark.hadoop.fs.s3a.secret.key\", s3_cfg[\"secret_key\"])\n",
    "        .set(\"spark.task.maxFailures\", \"1\")\n",
    "        # .setMaster(\"local[*]\")\n",
    "    )\n",
    "    builder = pyspark.sql.SparkSession.builder.config(conf=conf)\n",
    "    spark_session = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "    return spark_session\n",
    "\n",
    "\n",
    "def show_cfg(spark_session):\n",
    "    \"\"\"Print out the spark config.\"\"\"\n",
    "    cfg = spark_session.sparkContext.getConf().getAll()\n",
    "    for key, value in cfg:\n",
    "        if key in (\n",
    "            \"spark.submit.pyFiles\",\n",
    "            \"spark.driver.extraJavaOptions\",\n",
    "            \"park.app.initial.jar.urls\",\n",
    "            \"spark.files\",\n",
    "            \"spark.repl.local.jars\",\n",
    "            \"spark.app.initial.file.urls\" \"spark.executor.extraJavaOption\",\n",
    "            \"spark.app.initial.jar.urls\" \"spark.app.initial.file.urls\",\n",
    "        ):\n",
    "            print(key)\n",
    "            for csv in value.split(\",\"):\n",
    "                print(\"    \" + str(csv))\n",
    "        else:\n",
    "            print(f\"{key} = {value}\")\n",
    "\n",
    "\n",
    "def print_spark_info(sc: SparkSession):\n",
    "    \"\"\"Print some spark info.\"\"\"\n",
    "    cfg: pyspark.SparkConf = sc.sparkContext.getConf()\n",
    "    print(f'==== spark app: {cfg.get(\"spark.app.name\")} ====')\n",
    "    print(f'Spark master: {cfg.get(\"spark.master\")}')\n",
    "    print(f'Delta lake location: {cfg.get(\"spark.sql.warehouse.dir\")}')\n",
    "    print(f'S3 endpoint: {cfg.get(\"spark.hadoop.fs.s3a.endpoint\")}')\n",
    "\n",
    "    dbs = sc.catalog.listDatabases()\n",
    "    print(\"Databases:\")\n",
    "    for db in dbs:\n",
    "        print(db.name)\n",
    "        tables = sc.catalog.listTables(db.name)\n",
    "        for tbl in tables:\n",
    "            print(f\"    {tbl.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = get_spark_session(spark_name_base)\n",
    "aggregated_df = (\n",
    "    spark.table(\"bronze.wikipedia_page_reads\")\n",
    "    .groupBy(\"date\", \"page_title\", \"country_code\")\n",
    "    .agg(\n",
    "        _sum(col(\"count_views\")).alias(\"total_count_views\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "national_win = Window.partitionBy(\"date\", \"country_code\").orderBy(col(\"total_count_views\").desc())\n",
    "\n",
    "ranked_df = aggregated_df.withColumn(\"national_rank\", row_number().over(national_win))\n",
    "\n",
    "final_df = ranked_df.filter(col(\"national_rank\") <= 100).orderBy(\"date\", \"national_rank\", \"country_code\")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS silver\")\n",
    "table_name = \"silver.wikipedia_page_ranks_100\"\n",
    "(\n",
    "    final_df.write.mode(\"overwrite\")  # Options: 'overwrite', 'append', 'ignore', 'error' (default)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .format(\"delta\")  # Options: 'parquet', 'csv', 'json', 'orc', etc.\n",
    "    .partitionBy(\"date\")\n",
    "    .saveAsTable(table_name)\n",
    ")\n",
    "print_spark_info(spark)\n",
    "spark.stop()\n",
    "print(\"All done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFDS 3.8.10",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
