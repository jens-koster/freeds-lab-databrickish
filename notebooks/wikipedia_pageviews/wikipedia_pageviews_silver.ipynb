{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Wikipedia pageviews Silver\n",
    "Executes by rundate.\n",
    "Aggregates and ranks pageviews on a daily level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "execution_date = \"2025-05-15T01:00:00+00:00\"\n",
    "execution_id = f\"wikipedia_pageviews_silver-{dt.datetime.now():%Y%m%d-%H0000}\"\n",
    "full_refresh = False\n",
    "silver_db = \"silver\"\n",
    "bronze_db = \"bronze\"\n",
    "\n",
    "print(f\"Starting notebook execution: {execution_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from freeds.spark import get_spark_session\n",
    "from freeds.utils import date_range\n",
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "if not os.environ.get(\"SPARK_CONF_DIR\"):\n",
    "    os.environ[\"SPARK_CONF_DIR\"] = \"/opt/tfds/spark/conf\"\n",
    "\n",
    "\n",
    "source_table_name = f\"{bronze_db}.wikipedia_page_reads\"\n",
    "target_table_name = f\"{silver_db}.wikipedia_page_ranks_100\"\n",
    "\n",
    "spark = get_spark_session(execution_id)\n",
    "\n",
    "partition_list = date_range(execution_date=execution_date, length=2)\n",
    "\n",
    "filtered_df = spark.table(source_table_name).filter(col(\"date\").isin(partition_list))\n",
    "\n",
    "aggregated_df = filtered_df.groupBy(\"date\", \"page_title\", \"country_code\").agg(\n",
    "    _sum(col(\"count_views\")).alias(\"total_count_views\"),\n",
    ")\n",
    "\n",
    "national_win = Window.partitionBy(\"date\", \"country_code\").orderBy(col(\"total_count_views\").desc())\n",
    "\n",
    "ranked_df = aggregated_df.withColumn(\"national_rank\", row_number().over(national_win))\n",
    "\n",
    "final_df = ranked_df.filter(col(\"national_rank\") <= 100).orderBy(\"date\", \"national_rank\", \"country_code\")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {silver_db}\")\n",
    "if not full_refresh:\n",
    "    spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "(\n",
    "    final_df.write.mode(\"overwrite\")  # Options: 'overwrite', 'append', 'ignore', 'error' (default)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .format(\"delta\")  # Options: 'parquet', 'csv', 'json', 'orc', etc.\n",
    "    .partitionBy(\"date\")\n",
    "    .saveAsTable(target_table_name)\n",
    ")\n",
    "spark.stop()\n",
    "print(f\"All done: {execution_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFDS 3.8.10",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
